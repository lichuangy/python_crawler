# 爬虫介绍

通过编写程序，模拟浏览器上网，抓取网络上的数据的过程。

## 爬虫使用场景

爬虫使用场景可分为：**通用爬虫，聚焦爬虫，增量爬虫** 三种

### 通用爬虫

> 试图索引和下载网站上的所有页面,范围最大,覆盖整个网站。不针对特定主题或领域,会爬取网站上尽可能多的页面。搜索引擎的爬虫大多属于通用爬虫。

### 聚焦爬虫

> 爬取与特定主题或领域相关的页面,范围窄一些。例如只爬取与 machine learning 相关的页面,而不会爬取太多无关内容。可以提高爬取效率。

### 增量爬虫

> 主要爬取网站新增或更新的页面。通过定期爬取判断页面是否有更新,只获取新增的信息,以保持搜索引擎索引的实时性。增量爬虫可以大大减少重复爬取,提高效率。



## 爬虫的矛与盾

### 反爬机制

> 反爬机制是网站为了防止爬虫爬取数据而采取的一些技术手段,主要包括:
>
> - 用户验证:需要登录才能访问页面,如验证码、短信验证等。
> - 请求频率限制:同一IP请求过快会被限制或封禁。
> - User-Agent检测:检测User-Agent参数,识别爬虫请求。IP黑名单:记录并屏蔽常见爬虫的IP地址段。
> - 数据加密:对关键数据加密,提高爬取难度。

### 反反爬机制

> 反反爬机制是爬虫为了应对网站的反爬机制而采取的一些技术手段,主要包括:
>
> - IP代理池:使用大量代理IP,规避IP限制。
> - User-Agent伪装:使用常见浏览器的User-Agent。
> - 识别验证:使用机器视觉等技术自动识别验证码和验证码。
> - 动态爬取:使用WebSocket等技术实时爬取动态内容。
> - 分布式爬取:使用多台设备分布式爬取,分散风险。
> - 反爬检测:探测网站反爬机制,采取对应的策略。
> - - 爬取速率控制:控制爬取速度,减少被检测的可能性



## robots.txt协议

> 网站通过向爬虫表明哪些页面可以抓取,哪些页面不能抓取的一种通用标准。

robots协议的主要特点包括:

1. 它是一个txt文件,一般放在网站根目录下,命名为robots.txt。
2. 文件内使用Allow和Disallow等指令告知爬虫哪些页面可以访问,哪些页面不能访问。
3. 伦理爬虫在爬取网站前,都应该先请求和解析robots.txt文件,以遵守网站的爬取规则。
4. robots协议不是强制性的,不法分子可以选择性忽略它。
5. 它可以用于临时屏蔽某些页面,或者节流爬取频率等。
6. 主流搜索引擎爬虫像Google和Bing都会遵守robots协议的规则。
7. 网站可以通过robots.txt来优化搜索引擎抓取效果。
8. 机器人可以访问Disallow的页面,但需要遵循爬取间隔时间等限制。

## http协议

> HTTP(超文本传输协议)是互联网上应用最为广泛的一种网络协议,用于定义客户端和服务器之间进行数据交换的规则。

HTTP协议详解:

1. HTTP是基于TCP/IP协议之上的应用层协议,通常侦听80端口。
2. HTTP是无状态的协议,每次请求都需要建立新的连接。
3. HTTP请求方式有GET、POST、HEAD、PUT、DELETE等。GET用于获取资源,POST用于传输实体主体。
4. HTTP请求由请求行、请求头、空行和请求数据4部分组成。
5. HTTP响应由状态行、响应头、空行和响应数据4部分组成。状态码表示响应结果。
6. HTTP工作方式:客户端发起请求,服务器返回响应;一个请求对应一个响应。
7. HTTP可以传输多种数据格式,如HTML、XML、JSON、图像等。
8. HTTP是不安全的,通信使用明文,需要使用HTTPS(HTTP+SSL)来加密和认证。
9. HTTP/2相比HTTP/1.x优化了头压缩、多路复用等,提高传输性能。

### http请求

1. 请求行:包含请求方法、URL、HTTP版本
   例如:GET /index.html HTTP/1.1
2. 请求头:包含请求的相关信息,如User-Agent、Referer等
   例如:User-Agent: Mozilla/5.0
3. 空行:表示请求头结束
4. 请求数据:像POST请求放在请求体中的数据

请求头:

> - User-Agent: 发出请求的客户端程序信息。
> - Accept: 客户端能够接收的内容类型。
> - Accept-Language: 客户端偏好的语言。
> - Cookie: 存储客户端信息的键值对。
> - Referer: 请求来源页面的地址。
> - Authorization: HTTP认证信息。
> - Content-Type: 请求内容的媒体类型。
> - Content-Length: 请求内容的长度。

### http响应

1. 状态行:包含HTTP版本、状态码、状态信息
   例如:HTTP/1.1 200 OK
2. 响应头:包含响应的相关信息,如Content-Type等
   例如:Content-Type: text/html
3. 空行:表示响应头结束
4. 响应数据:服务器返回给客户端的文本、 html、图片等数据

响应头:

> - Server:响应请求的服务器软件信息。
> - Set-Cookie: 服务端设置的Cookie信息。
> - Content-Type: 响应内容的媒体类型。
> - Content-Length: 响应内容的长度。
> - Content-Encoding: 对响应内容的编码方式。
> - Cache-Control: 控制缓存行为。
> - Expires: 响应过期的时间。
> - Location: 重定向目标URL。



## Https协议

> HTTPS(全称Hyper Text Transfer Protocol Secure)是HTTP的安全版,在HTTP的基础上通过SSL/TLS层进行加密和认证,防止数据在网络传输过程中被中间人窃听或篡改。



### 对称加密

对称加密(秘钥加密):

> - 加密和解密使用同一把密钥。
> - 代表算法有DES、AES等。
> - 加解密速度快,适合大数据量的加密。
> - 密钥管理和交换难点。

对称加密可以比喻为“一把车锁只能用一把钥匙开启”。

只有配对的钥匙才能将其上锁和解锁一样,对称加密也只有通过原来的密钥,才能对密文进行解密,获取明文。
没有相应的密钥,即使知道加密算法,也无法破解对称加密。

### 非对称加密

2. 非对称加密(公开密钥加密):

- 加密和解密使用不同的公钥和私钥。
- 代表算法有RSA等。
- 计算复杂,加解密速度较慢。
- 解决了密钥管理问题。
- 通常用于传送对称加密密钥。

非对称就是加密和解密是两个密钥，而且是相对的，例如：快递保证用胶带封口，拆快递用刀划开

- 加密就是用胶带封口
- 解密就是用刀划开胶带

> 这两个动作需要不同的“钥匙”(胶带和刀),且必须配对使用,才能完成整个信息保密传递的过程。
> 如果只有胶带没有刀,就无法打开;如果只有刀没有胶带封口,也无法保证内容的机密性。
> 非对称加密正是利用这种“封锁”和“打开”的対称操作,但需要不同的密钥来实现。

### 证书加密

证书加密是一种基于数字证书的加密机制,通常应用于网络通信安全领域。其基本原理如下:

1. 数字证书中包含持有者的公钥。
2. 证书由受信任的证书颁发机构(CA)签署,以保证公钥的真实可靠。
3. 通信双方都持有可验证对方身份的数字证书。
4. 发送方使用接收方的公钥加密信息,接收方用自己的私钥解密。
5. 通过证书链验证公钥的有效性,确保通信安全。
   与传统非对称加密相比,证书加密的关键创新是引入数字证书和公钥基础设施(PKI)。
   这消除了公钥的来源认证问题,有效防止中间人攻击。

证书密钥加密就是中间有一个可信可靠的中间人

所以整个过程中,CA充当了一个“可信第三方”,验证并认证了公钥的真实性。这避免了非对称加密中的公钥来源不明风险。



# requests模块



## request模块的安装与使用

### 环境安装

```bash
	#安装
pip install requests
```

## requests模块简介

> Python requests模块是用于发送HTTP网络请求的强大库,常用于爬取网页、接口测试等,主要功能和常用方法如下:
>
> 1. GET请求:requests.get() 发送GET请求,获取资源。
> 2. POST请求:requests.post() 发送POST请求,传输数据。
> 3. 请求头设置:通过headers参数设置User-Agent、Referer等头信息,伪装请求来源。
> 4. 参数传递:params用于GET请求参数,data用于POST请求数据,files用于上传文件。
> 5. 响应内容:text属性获取字符串形式响应,content获取二进制响应,json()获取JSON格式响应。
> 6. 状态码检查:status_code属性获取HTTP响应状态码,可以判断请求是否成功。
> 7. 请求地址处理:params参数自动处理参数,urljoin()可以拼接URL。
> 8. 会话管理:Session对象可以做到会话维持,在跨请求时保存某些参数。
> 9. 证书验证:可以使用verify参数控制证书验证,忽略不安全警告。
> 10. 代理设置:通过proxies参数设置HTTP和SOCKS代理。
> 11. 超时设置:timeout参数设置请求超时时间,以秒为单位。
> 12. 身份认证:基本认证、摘要认证等,通过auth参数完成。
> 13. 文件上传:直接通过files参数上传文件。
> 14. 异常处理:网络请求中常见的各类异常。



## 简单使用

1. 导入requests模块
2. 指定url地址
3. 发起请求
4. 获取响应数据
5. 解析页面
6. 根据需求提取数据
7. 存储数据结果

### 第一个爬虫程序

```python
# 需求：第一个爬虫程序，爬取搜狗首页

# 1. 导入requests模块
import requests

def requests_test():
    # 2. 指定url地址
    path = "https://www.sogou.com"
    # 3. 发起请求
    content = requests.get(url=path)
    # 4. 获取响应数据
    response = content.text
    print(requests)
    # 5. 存储数据结果
    with open('./sogo.html','w',encoding='utf-8') as f:
        f.write(response)
    print('保存结束')
if __name__ == '__main__':
    requests_test()
```

## 代码简单实战

### requests网页采集器

```python
 # 导包
import requests

# path = 'https://www.sogou.com/web?query=是小李同学&'
path = 'https://www.sogou.com/web'

# 动态输入参数
kw = input("please input you query word:")
# 处理url携带的参数，封装到字典中
params = {'query':kw}
# 对指定的url发起请求对应的url是携带参数的，并且请求过程中处理了参数
response = requests.get(url=path,params=kw)
content = response.text
print(content)

# 结果存储
filename = kw + '.html'
with open(filename,'w',encoding='utf-8') as f:
    f.write(content) f.write(content)
```

### User-Agent伪装

User-Agent简介：

```
ser-Agent(用户代理)是HTTP协议中的一个头字段,它用于表示请求是由什么客户端程序发出的。
User-Agent字段包含了客户端程序的应用类型、操作系统、软件开发商以及版本号等信息。
```

作用：

```
1. 标识客户端信息
	User-Agent字符串包含使用的浏览器名称、版本、操作系统等信息,服务器端可以解析该字符串来识别客户端信息。
2. 调整内容展示
   网站可以根据User-Agent提供不同的网页版本,以适应移动端或者不同浏览器的展示效果。
3. 统计分析
   可以通过分析User-Agent统计不同用户使用的客户端信息,做出客群分析。
4. 辨别爬虫行为
   网站可以检测可疑的User-Agent来发现爬虫程序。
```

why进行伪装：

```
1. 伪装浏览器避免被识破
	有些网站会阻止没有浏览器身份标识的请求,所以需要伪装成浏览器的User-Agent。
2. 访问仅针对特定客户端的资源
	一些网站会对搜索爬虫和移动用户提供不同版本的页面,需要通过User-Agent来获取。
3. 防止被识别为爬虫而被封禁
	一些网站会检测可疑的爬虫User-Agent并做封禁,伪装可以避免被检测。
4. 统计分析不准确
	使用真实的User-Agent会使网站统计到错误的用户信息,伪装可以避免这种情况。
	综上,User-Agent伪装可以帮助程序以浏览器的身份访问网站,获得更好的适配效果,也有助于隐藏程序行为避免被检测。
```

代码：

```python
# 导包
import requests

# path = 'https://www.sogou.com/web?query=是小李同学&'
path = 'https://www.sogou.com/web'

# 动态输入参数
kw = input("please input you query word:")
# 处理url携带的参数，封装到字典中
params = {'query':kw}

#US伪装，封装到headers字典中
headers = {
    "User-Agent":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
}
# 对指定的url发起请求对应的url是携带参数的，并且请求过程中处理了参数，UA伪装
response = requests.get(url=path,params=params,headers=headers)
content = response.text
print(content)

# 结果存储
filename = kw + '.html'
with open(filename,'w',encoding='utf-8') as f:
    f.write(content)
print(filename,'保存成功！')
```

### 百度翻译破解

破解步骤;

1. 分析百度翻译页面的网络请求,找到翻译接口的URL地址,通常是POST请求。
2. 通过浏览器调试工具(F12)监听Ajax请求,查看接口需要传递的参数(一般是查询词词组、目标语言等)。
3. 构造请求参数,模拟Ajax请求的参数,放入requests.post的参数data中。
4. 添加请求头,设置User-Agent来模拟浏览器,避免被封IP。
5. 发送请求,获取响应内容,多为JSON格式的数据，在响应头的Content-Type 后面可看到响应格式类型。
6. 对响应数据进行解析,提取出需要的翻译结果。
7. 将翻译结果保存到本地或数据库等。

```python
# 导入requests和json模块
import requests  
import json

# 目标接口URL
path = 'https://fanyi.baidu.com/sug'  

# 用户输入查询关键词  
kw = input("请输入查询关键词:")

# 构造请求参数
params = {'kw':kw}  

# 添加请求头,伪装为浏览器
headers = {"User-Agent": "Mozilla/5.0"}

# 发送POST请求  
response = requests.post(url=path, data=params, headers=headers)

# 获取响应内容并解析成JSON格式
content = json.loads(response.text)  

# 打印响应数据类型和内容
print(type(content)) 
print(content)

# 打开文件准备写入
filename = kw + '.json'
with open(filename, 'w', encoding='utf-8') as f:
    # 将JSON格式的响应内容转成字符串
    content_str = json.dumps(content, ensure_ascii=False) 

    # 将字符串写入文件
    f.write(content_str)
    
print(filename + ' 保存成功!')
```

通过构造参数,伪装请求头,使用requests模块发送POST请求,获取并解析响应数据,最后保存到文件,就实现了简单的百度翻译接口破解。



# Python正则表达式

# re模块详解

## 一、re模块介绍

re模块是Python内置的正则表达式模块,提供了丰富的正则表达式匹配操作。

### 1.1 re模块的优点

- 语法简洁,功能强
- 大支持复杂的正则表达式
- 函数接口简单易用

### 1.2 re模块的常用函数re模块常用的函数包括:

- re.match - 从字符串开头匹配正则表达式
- re.search - 在字符串中搜索匹配正则表达式的第一个位置
- re.findall - 找到字符串中所有的匹配,返回结果列表
- re.split - 根据正则表达式对字符串进行分割
- re.sub - 在字符串中替换匹配项
- re.compile - 编译正则表达式字符串,获得正则表达式对象

## 二、re.match方法

### 2.1 基本语法re.match尝试从字符串的起始位置匹配一个模式,语法格式如下:

```
python
re.match(pattern, string, flags=0)
```

参数:

- pattern: 匹配的正则表达式 
  \- string: 要匹配的字符串
  \- flags: 正则表达式使用时的控制标记### 2.2 示例

```
python
import re

match = re.match('www','www.runoob.com')  
# 成功匹配

match = re.match('com','www.runoob.com')
# 未成功匹配  
```

## 三、re.search方法

### 3.1 基本语法re.search扫描整个字符串,返回第一个成功的匹配。

```
python
re.search(pattern, string, flags=0)
```

### 3.2 示例

```
python
import re

match = re.search('www','www.runoob.com')  # 成功匹配

match = re.search('com','www.runoob.com') # 成功匹配  
```

## 四、re.findall方

### 4.1 基本语法re.findall搜索字符串,以列表形式返回全部能匹配的子串。

```
python 
re.findall(pattern, string, flags=0)
```

### 4.2 示例

```
python
import re

result = re.findall('ab','ab abcd ab')
# ['ab', 'ab']
```



## 五、代码展示

```python
"""

Python中的re模块提供了正则表达式的各种操作函数,常见和常用的方法包括:
- re.match - 从字符串开头匹配正则表达式,返回匹配对象
- re.search - 在字符串中搜索匹配正则表达式的第一个位置,返回匹配对象
- re.findall - 查找字符串中所有的匹配,返回列表
- re.split - 根据正则表达式分割字符串,返回列表
- re.sub - 在字符串中替换匹配正则表达式的内容
- re.compile - 编译正则表达式字符串,返回正则表达式对象
- re.fullmatch - 完全匹配整个字符串,返回匹配对象
- re.finditer - 查找字符串中所有的匹配,返回迭代器
- re.purge - 清空正则表达式的缓存
- re.escape - 将特殊字符转义,用于匹配字符串字面值
此外,匹配对象也有一些常用的方法:
- group() - 返回匹配内容
- start() - 返回匹配内容在原字符串的开始索引
- end() - 返回匹配内容在原字符串的结束索引
- span() - 返回匹配内容在原字符串的开始和结束索引

"""
import re

string = 'Hello World!'

print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~match 规则：从字符串开头匹配正则表达式,返回匹配对象~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
pattern = r"Hello"
# 情况1:正则匹配字符串开头,匹配成功
m1 = re.match(pattern,string)

# 情况2:正则不匹配字符串开头,匹配失败
m2 = re.match(r"World",string)

# 情况3:正则匹配字符开头0次或者多次,匹配成功
m3 = re.match(r"\w*",string)

# 情况4:^正则匹配开头,匹配成功
m4 = re.match(pattern, string)

print(m4)


print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~search 规则：在字符串中搜索匹配正则表达式的第一个位置,返回匹配对象~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

# 情况1:正则匹配字符串中间部分,成功匹配
s1 = re.search(r"World",string)

# 情况2:正则模式匹配字符串开头,也能成功匹配
s2 = re.search(pattern,string)

# 情况3:使用^锚定开头,匹配失败
pattern = r"^World"
s3 = re.search(pattern,string)

# 情况5:正则匹配字符串末尾,匹配成功
s4 = re.search(r'!',string)

# 情况6:使用$锚定结尾,匹配成功
s5 = re.search(r'!$',string)

print(s5)


print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~findall 规则：查找字符串中所有的匹配,返回列表~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

string = 'Hello Hello World'
# 情况1:正常匹配,返回列表
fa1 = re.findall(r'Hello',string)

# 情况2:正则没有匹配,返回空列表
pattern = r'abc'
fa2 = re.findall(pattern, string) # []

# 情况3:使用*重复匹配,返回列表
pattern = r'Hel*o'
fa3 = re.findall(pattern, string) # ['Hello', 'Hello']

# 情况4:正则包含组,(())分组匹配
pattern = r'(Hello) World'
fa4 = re.findall(pattern, string) # ['Hello']

print(fa4)


print("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~split, sub~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

string = 'Hello World! 123 456'

# split - 根据正则表达式分割字符串,返回列表
print(re.split(r'\s+', string))
# ['Hello', 'World!', '123', '456']

# sub - 在字符串中替换匹配正则表达式的内容
print(re.sub(r'\d+', 'number', string))
# Hello World! number number

```

以上就是re模块的详细介绍,包括匹配方法的基本语法、示例等内容。可以用来温故re模块的正则表达式知识。



# 数据解析

爬虫爬取网页后的内容提取过程。它的主要作用是从爬取的网页中提取出需要的目标数据。

## 数据解析分类

1. 正则表达式解析
   使用正则表达式来匹配网页内容,并提取出需要的数据。适用于有规律的文本解析。

2. XPath解析
   使用XPath表达式来选取HTML文档中的元素,并获取元素的数据。

3. CSS选择器解析
   使用CSS选择器来选择元素,并获取数据。可以配合Beautiful Soup等库使用。

4. JSON解析
   对于API返回的JSON格式数据,可以直接使用json模块解析。

5. XML解析
   使用ElementTree等库解析XML文档,获取数据。

6. bs4 解析

   BeautifulSoup(简称bs4)是Python中常用的网页解析库。



### 正则解析

1.正则表达式

| 字符                                                       | 描述                                                         |
| :--------------------------------------------------------- | ------------------------------------------------------------ |
| \                                                          | 将下一个字符标记为一个特殊字符、或一个原义字符、或一个 向后引用、或一个八进制转义符。例如，'n' 匹配字符 "n"。'\n' 匹配一个换行符。序列 '\\' 匹配 "\" 而 "\(" 则匹配 "("。 |
| ^                                                          | 匹配输入字符串的开始位置。如果设置了 RegExp 对象的 Multiline 属性，^ 也匹配 '\n' 或 '\r' 之后的位置。 |
| $                                                          | 匹配输入字符串的结束位置。如果设置了RegExp 对象的 Multiline 属性，$ 也匹配 '\n' 或 '\r' 之前的位置。 |
| *                                                          | 匹配前面的子表达式零次或多次。例如，zo* 能匹配 "z" 以及 "zoo"。* 等价于{0,}。 |
| +                                                          | 匹配前面的子表达式一次或多次。例如，'zo+' 能匹配 "zo" 以及 "zoo"，但不能匹配 "z"。+ 等价于 {1,}。 |
| ?                                                          | 匹配前面的子表达式零次或一次。例如，"do(es)?" 可以匹配 "do" 或 "does" 。? 等价于 {0,1}。 |
| {n}                                                        | n 是一个非负整数。匹配确定的 n 次。例如，'o{2}' 不能匹配 "Bob" 中的 'o'，但是能匹配 "food" 中的两个 o。 |
| {n,}                                                       | n 是一个非负整数。至少匹配n 次。例如，'o{2,}' 不能匹配 "Bob" 中的 'o'，但能匹配 "foooood" 中的所有 o。'o{1,}' 等价于 'o+'。'o{0,}' 则等价于 'o*'。 |
| {n,m}                                                      | m 和 n 均为非负整数，其中n <= m。最少匹配 n 次且最多匹配 m 次。例如，"o{1,3}" 将匹配 "fooooood" 中的前三个 o。'o{0,1}' 等价于 'o?'。请注意在逗号和两个数之间不能有空格。 |
| ?                                                          | 当该字符紧跟在任何一个其他限制符 (*, +, ?, {n}, {n,}, {n,m}) 后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串 "oooo"，'o+?' 将匹配单个 "o"，而 'o+' 将匹配所有 'o'。 |
| .                                                          | 匹配除换行符（\n、\r）之外的任何单个字符。要匹配包括 '\n' 在内的任何字符，请使用像"**(.\|\n)**"的模式。 |
| (pattern)                                                  | 匹配 pattern 并获取这一匹配。所获取的匹配可以从产生的 Matches 集合得到，在VBScript 中使用 SubMatches 集合，在JScript 中则使用 $0…$9 属性。要匹配圆括号字符，请使用 '\(' 或 '\)'。 |
| (?:pattern)                                                | 匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 "或" 字符 (\|) 来组合一个模式的各个部分是很有用。例如， 'industr(?:y\|ies) 就是一个比 'industry\|industries' 更简略的表达式。 |
| (？=pattern)                                               | 正向肯定预查（look ahead positive assert），在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，"Windows(?=95\|98\|NT\|2000)"能匹配"Windows2000"中的"Windows"，但不能匹配"Windows3.1"中的"Windows"。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 |
| (?!pattern)                                                | 正向否定预查(negative assert)，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如"Windows(?!95\|98\|NT\|2000)"能匹配"Windows3.1"中的"Windows"，但不能匹配"Windows2000"中的"Windows"。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 |
| (?<=pattern)                                               | 反向(look behind)肯定预查，与正向肯定预查类似，只是方向相反。例如，"`(?<=95|98|NT|2000)Windows`"能匹配"`2000Windows`"中的"`Windows`"，但不能匹配"`3.1Windows`"中的"`Windows`"。 |
| (?<!pattern)                                               | 反向否定预查，与正向否定预查类似，只是方向相反。例如"`(?<!95|98|NT|2000)Windows`"能匹配"`3.1Windows`"中的"`Windows`"，但不能匹配"`2000Windows`"中的"`Windows`"。 |
| x\|y                                                       | 匹配 x 或 y。例如，'z\|food' 能匹配 "z" 或 "food"。'(z\|f)ood' 则匹配 "zood" 或 "food"。 |
| [xyz]                                                      | 字符集合。匹配所包含的任意一个字符。例如， '[abc]' 可以匹配 "plain" 中的 'a'。 |
| [^xyz]                                                     | 负值字符集合。匹配未包含的任意字符。例如， '[^abc]' 可以匹配 "plain" 中的'p'、'l'、'i'、'n'。 |
| [a-z]                                                      | 字符范围。匹配指定范围内的任意字符。例如，'[a-z]' 可以匹配 'a' 到 'z' 范围内的任意小写字母字符。 |
| [^a-z]                                                     | 负值字符范围。匹配任何不在指定范围内的任意字符。例如，'[^a-z]' 可以匹配任何不在 'a' 到 'z' 范围内的任意字符。 |
| [\b](https://www.runoob.com/regexp/regexp-metachar-b.html) | 匹配一个单词边界，也就是指单词和空格间的位置。例如， 'er\b' 可以匹配"never" 中的 'er'，但不能匹配 "verb" 中的 'er'。 |
| [\B](https://www.runoob.com/regexp/regexp-metachar-b.html) | 匹配非单词边界。'er\B' 能匹配 "verb" 中的 'er'，但不能匹配 "never" 中的 'er'。 |
| \cx                                                        | 匹配由 x 指明的控制字符。例如， \cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 'c' 字符。 |
| \d                                                         | 匹配一个数字字符。等价于 [0-9]。                             |
| \D                                                         | 匹配一个非数字字符。等价于 [^0-9]。                          |
| \f                                                         | 匹配一个换页符。等价于 \x0c 和 \cL。                         |
| \n                                                         | 匹配一个换行符。等价于 \x0a 和 \cJ。                         |
| \r                                                         | 匹配一个回车符。等价于 \x0d 和 \cM。                         |
| \s                                                         | 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 |
| \S                                                         | 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。                  |
| \t                                                         | 匹配一个制表符。等价于 \x09 和 \cI。                         |
| \v                                                         | 匹配一个垂直制表符。等价于 \x0b 和 \cK。                     |
| \w                                                         | 匹配字母、数字、下划线。等价于'[A-Za-z0-9_]'。               |
| \W                                                         | 匹配非字母、数字、下划线。等价于 '[^A-Za-z0-9_]'。           |
| \xn                                                        | 匹配 n，其中 n 为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，'\x41' 匹配 "A"。'\x041' 则等价于 '\x04' & "1"。正则表达式中可以使用 ASCII 编码。 |
| \num                                                       | 匹配 num，其中 num 是一个正整数。对所获取的匹配的引用。例如，'(.)\1' 匹配两个连续的相同字符。 |
| \n                                                         | 标识一个八进制转义值或一个向后引用。如果 \n 之前至少 n 个获取的子表达式，则 n 为向后引用。否则，如果 n 为八进制数字 (0-7)，则 n 为一个八进制转义值。 |
| \nm                                                        | 标识一个八进制转义值或一个向后引用。如果 \nm 之前至少有 nm 个获得子表达式，则 nm 为向后引用。如果 \nm 之前至少有 n 个获取，则 n 为一个后跟文字 m 的向后引用。如果前面的条件都不满足，若 n 和 m 均为八进制数字 (0-7)，则 \nm 将匹配八进制转义值 nm。 |
| \nml                                                       | 如果 n 为八进制数字 (0-3)，且 m 和 l 均为八进制数字 (0-7)，则匹配八进制转义值 nml。 |
| \un                                                        | 匹配 n，其中 n 是一个用四个十六进制数字表示的 Unicode 字符。例如， \u00A9 匹配版权符号 (?)。 |

正则表达式——贪婪匹配

> 贪婪匹配是指正则表达式会尽可能匹配最长的子字符串。
> 例如正则表达式 "a+" 对字符串 "aaaa" 进行匹配时:贪婪匹配:
> 整个字符串"aaaa"都会被匹配。



正则表达式——非贪婪匹配

> 正则表达式会尽可能少的匹配字符。可以在量词后面加上"?"实现非贪婪匹配。
>
> 例如:正则表达式 "a+?" 对字符串 "aaaa" 进行匹配,会匹配最小个数的 "a",返回结果是:"a"。



2.正则解析步骤

​	①.获取网页源代码

​	②.分析源码,编写正则表达式

​	③.使用re模块执行匹配

​	④.处理匹配结果

​	⑤.存储和分析数据

3.代码

```python
import requests
import re
import os

# 导入需要的模块

# <dd>主人,你就可怜可怜我吧!!<br><img src="https://pic.biedoul.com/Uploads/Images/34/595e2100b2af2.jpg"><br></dd>
# 需求:爬取别逗网站中图片模块下的所有图片
if __name__ == '__main__':

    # 判断作为主程序运行

    # 创建存放图片目录
    if not os.path.exists('./biedou/pic'):
        os.makedirs('./biedou/pic')

    # 检查目录是否存在,不存在则创建目录

    # 爬取地址
    url = 'https://www.biedoul.com/pic/'

    # 定义要爬取的网址

    # 代理UA
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'
    }

    # 设置请求头信息
    for page_num in range(1,3):
        new_url = url + str(page_num)
        page_text = requests.get(new_url, headers=headers).text

        # 使用requests获取网页内容

        # 使用聚焦爬虫将页面中的所有图片进行解析/提取
        ex = re.compile(r'<DD>.*?<br><img src="(.*?)"/><br></DD>', re.S)
        image_list = re.findall(ex, page_text)

        # 使用正则表达式提取图片链接

        for i in image_list:
            image_data = requests.get(i, headers=headers).content
            with open('./biedou/pic/' + str(page_num) + '_' + i.split('/')[-1], 'wb') as f:
                f.write(image_data)
    # 遍历图片链接,请求图片内容并保存文件
```



### bs4解析beautiful soup

#### 一、数据解析的原理：

​		1.标签定位

​		2.提取标签、标签属性中存储的数据值

#### 二、bs4数据解析的原理：

​		1.实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中

​		2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取

#### 三、环境安装：

​		pip install bs4

​		pip install lxml 

#### 四、实例化BeautifulSoup对象

​		from bs4 import BeautifulSoup

​		对象实例化

​			1.  本地的html文档中的数据加载到该对象中：

​				fp = open('docname.html','r',encodeing='utf-8')

​				soup = BeautifulSoup(fp,'lxml')

​			2. 获取网络网页源码加载到该对象中 

​				page_text = requests.text

​				soup = BeautifulSoup(page_text,'lxml')

#### 五、bs4数据解析的方法与属性

1. soup.tagName

   返回文档中第一次出现的tagName对应的标签

2. soup.find()

   soup.find('a')等价于soup.a,通过tagName对应的标签查找

   soup.find('div',class_/id/attr='navtop') 通过标签的属性找到tagName对于标签的查找

   soup.find_all('tagName') 返回一个列表，里面包含所有符合要求的标签

3. soup.select()

   soup.select('标签名，class，id，属性，组合，索引，内容')，返回一个列表

   索引定位：

   ​	soup.select.a[1]，如果有多个，需要选择第二个，则用索引进行选择

   层级选择器：

   ​	soup.select('标签，..' > ul > li > a') >表示一个层级

   ​	soup.select('.class > ul a') 空格表示多个层级

   获取标签之间的文本数据：

   ​	text/get_text():可以获取某一个标签中的所有文本内容

   ​	string:只获取该标签下面直系的文本内容

   获取标签的属性：

   ​	soup.a['href']

   

### xpath解析

最常用且最便捷高效的一种解析方式，通用性。

#### xpath解析原理：

​		1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。

​		2.调用etree对象中的xpath方法结合这xpath表达式实现标签的定位和内容的捕获。

#### 环境的安装：

​		pip install lxml

#### 实例化一个etree对象：

​		from lxml import etree

#### 将本地的html文档中的源码数据加载到etree对象中：

​		etree.parse(filepath)

​	获取网页源码加载到etree对象中:

​		etree.HTML('page_text')

#### xpath('xpath表达式')

​	/:从根节点开始定位，并表示一个层级。xpath('/html/body/div')

​	//:表示多个层级，可以表示从任意位置开始定位。xpath('/html///div')

​	属性定位：xpath('//div[@class = 'gushici]')

​	索引定位：xpath('//div[@class = "shici-pic-box"]/p')，索引是从1开始的。

​	取文本：/text() 和//text()

​		获取a标签直系下的文本内容xpath('//div[@class = "shici-pic-box"]/p[1]/a/text()')

​		获取a标签下所有的文本内容 xpath('//div[@class = "shici-pic-box"]/p[1]/a//text()')

​	取属性：/@attrName  例如img/src/href 

​		获取a直系下的所有a标签中第一个索引的href的属性的值：

​			xpath('//div[@class = "shici-pic-box"]/div/a[1]/@href')



#### xpath其他高级用法

1. 使用轴选取节点例如使用ancestor轴选取当前节点的所有先辈节点:

```
python
ancestors = tree.xpath('//a[@href]/ancestor::*')  
```

2. 使用谓语过滤节点例如选择前3个p节点:

```
python
p_nodes = tree.xpath('//p[position()<=3]')
```

3. 使用内置函数例如使用contains()判断节点文本是否包含某文字:

```
python
nodes = tree.xpath('//p[contains(text(),"老眼")]')
```

4. 定义变量和函数

```
python
poem_title = '//div[@class="gushici-box"]/p/b/text()'

def get_text(t):
  return t.strip()

titles = [get_text(t) for t in tree.xpath(poem_title)]
```

5. 使用命名空间可以在XPATH中加上前缀如`ns:`表示命名空间。
6. 模式匹配

```
python 
hrefs = tree.xpath('//*[local-name()="a"]/@href') 
```

7. 条件表达式

```
python
cond = tree.xpath('if //img then //img/@src else //a/text()')
```

[更多使用方法](https://www.w3school.com.cn/xpath/index.asp)

8. 代码

   ```html
   <-- html文件-->
   <!DOCTYPE html>
   <html>
   <head>
   <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
   
   <meta content="url=(0043)https://m.gushici.net/shici/579/579409.html" name="scraped from"/>
   
   <meta http-equiv="Cache-Control" content="no-siteapp"/>
   <meta http-equiv="Cache-Control" content="no-transform "/>
   <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes"/>
   <meta name="format-detection" content="telephone=no"/>
   <meta content="yes" name="apple-mobile-web-app-capable"/>
   <meta content="black" name="apple-mobile-web-app-status-bar-style"/>
   <title>郭公辅挽诗原文_龚诩_古诗词网</title>
   
   <meta name="keywords" content="郭公辅挽诗"/>
   <meta name="description" content="古娄樽酒忆相违,瞬息那知往事非。 生死可怜客易别,交游转觉老成稀。 谪仙此日骑鲸去,丁令何年化鹤归。 老眼一泓怀旧泪,随风散作雨霏霏。"/>
   
   <script src="./郭公辅挽诗原文_龚诩_古诗词网_files/hm.js.下载"></script>
   <script src="./郭公辅挽诗原文_龚诩_古诗词网_files/push.js.下载" id="ttzz"></script>
   <script src="./郭公辅挽诗原文_龚诩_古诗词网_files/jquery.min.js.下载"></script>
   
   <link href="./郭公辅挽诗原文_龚诩_古诗词网_files/style.css" rel="stylesheet" />
   
   <script src="./郭公辅挽诗原文_龚诩_古诗词网_files/gushi.js.下载"></script>
   
   </head>
   <body>
   
   <div class="head">
   	<a href="https://m.gushici.net/"><b>古诗词网</b></a>
   </div>
   <div class="navtop">
   	<a href="https://m.gushici.net/">推荐</a>
   	<a class="active" href="https://m.gushici.net/shici/">诗词</a>
   	<a href="https://m.gushici.net/mingju/">名句</a>
   	<a href="https://m.gushici.net/zuozhe/">作者</a>
   	<a href="https://m.gushici.net/book/">古籍</a>
   </div>
   <div class="search">
   <form action="https://m.gushici.net/search/s.php" method="get" name="searchform">
   	<div class="input-css">
   	<select name="type" class="sel" id="selectLeixing" onchange="ChangeSelectLX()">
   		<option value="all">综合</option>
   		<option value="shici" selected="">诗词</option>
   		<option value="biaoti">诗题</option>
   		<option value="ju">诗句</option>
   		<option value="zuozhe">作者</option>
   		<option value="book">古籍</option>
   
   	</select>	
   	<input type="text" class="wds" placeholder="作者/诗词/诗句" name="key"/>
   	</div>
   	<button class="button-css" type="submit">搜索</button>
   </form>
   </div>
   <div class="main">
   	<div class="gushici">
   		<div class="gushici-box">
   			<h1 style="font-size:22px; line-height:30px; margin-bottom:10px;">郭公辅挽诗</h1>
   			<p class="source"><a href="https://m.gushici.net/t/1/179/">明代</a><span>：</span><a href="https://m.gushici.net/t/16/16868/">龚诩</a></p>
   			<div class="gushici-box-text">
   				古娄樽酒忆相违，瞬息那知往事非。<br/>生死可怜客易别，交游转觉老成稀。<br/>谪仙此日骑鲸去，丁令何年化鹤归。<br/>老眼一泓怀旧泪，随风散作雨霏霏。</div>
   		</div>
   	</div>
   	<div class="shici">
   		<div class="fanyi check-more">
   				<div style="height:30px; font-weight:bold; font-size:16px; margin-bottom:10px; clear:both;">
   				<h2><span style="float:left;">词典释义</span></h2><span style="font-size:14px;color:#65645F;font-weight:normal;">（系统自动检索释义，仅供参考。）</span>
   				</div>
   			<span class="wordLink" onclick="ShowWordNote(472300, &quot;poem_title_1&quot;)"><span class="bold">古</span></span><span class="wordLink" onclick="ShowWordNote(484267, &quot;poem_title_1&quot;)"><span class="bold">娄</span></span><span class="wordLink" onclick="ShowWordNote(1018, &quot;poem_title_1&quot;)"><span class="bold">樽酒</span></span><span class="wordLink" onclick="ShowWordNote(482417, &quot;poem_title_1&quot;)"><span class="bold">忆</span></span><span class="wordLink" onclick="ShowWordNote(2480, &quot;poem_title_1&quot;)"><span class="bold">相违</span></span>，<span class="wordLink" onclick="ShowWordNote(4689, &quot;poem_title_1&quot;)"><span class="bold">瞬息</span></span><span class="wordLink" onclick="ShowWordNote(420346, &quot;poem_title_1&quot;)"><span class="bold">那知</span></span><span class="wordLink" onclick="ShowWordNote(186, &quot;poem_title_1&quot;)"><span class="bold">往事</span></span><span class="wordLink" onclick="ShowWordNote(473742, &quot;poem_title_1&quot;)"><span class="bold">非</span></span>。<div id="poem_title_1_comment" style="display:block"></div><span class="wordLink" onclick="ShowWordNote(512, &quot;poem_title_2&quot;)"><span class="bold">生死</span></span><span class="wordLink" onclick="ShowWordNote(27, &quot;poem_title_2&quot;)"><span class="bold">可怜</span></span><span class="wordLink" onclick="ShowWordNote(473901, &quot;poem_title_2&quot;)"><span class="bold">客</span></span><span class="wordLink" onclick="ShowWordNote(473462, &quot;poem_title_2&quot;)"><span class="bold">易</span></span><span class="wordLink" onclick="ShowWordNote(472729, &quot;poem_title_2&quot;)"><span class="bold">别</span></span>，<span class="wordLink" onclick="ShowWordNote(851, &quot;poem_title_2&quot;)"><span class="bold">交游</span></span><span class="wordLink" onclick="ShowWordNote(357719, &quot;poem_title_2&quot;)"><span class="bold">转觉</span></span><span class="wordLink" onclick="ShowWordNote(1866, &quot;poem_title_2&quot;)"><span class="bold">老成</span></span><span class="wordLink" onclick="ShowWordNote(476703, &quot;poem_title_2&quot;)"><span class="bold">稀</span></span>。<div id="poem_title_2_comment" style="display:block"></div><span class="wordLink" onclick="ShowWordNote(888, &quot;poem_title_3&quot;)"><span class="bold">谪仙</span></span><span class="wordLink" onclick="ShowWordNote(472582, &quot;poem_title_3&quot;)"><span class="bold">此</span></span><span class="wordLink" onclick="ShowWordNote(472203, &quot;poem_title_3&quot;)"><span class="bold">日</span></span><span class="wordLink" onclick="ShowWordNote(3579, &quot;poem_title_3&quot;)"><span class="bold">骑鲸</span></span><span class="wordLink" onclick="ShowWordNote(472285, &quot;poem_title_3&quot;)"><span class="bold">去</span></span>，<span class="wordLink" onclick="ShowWordNote(14237, &quot;poem_title_3&quot;)"><span class="bold">丁令</span></span><span class="wordLink" onclick="ShowWordNote(472695, &quot;poem_title_3&quot;)"><span class="bold">何</span></span><span class="wordLink" onclick="ShowWordNote(472533, &quot;poem_title_3&quot;)"><span class="bold">年</span></span><span class="wordLink" onclick="ShowWordNote(6945, &quot;poem_title_3&quot;)"><span class="bold">化鹤</span></span><span class="wordLink" onclick="ShowWordNote(482537, &quot;poem_title_3&quot;)"><span class="bold">归</span></span>。<div id="poem_title_3_comment" style="display:block"></div><span class="wordLink" onclick="ShowWordNote(1424, &quot;poem_title_4&quot;)"><span class="bold">老眼</span></span><span class="wordLink" onclick="ShowWordNote(3669, &quot;poem_title_4&quot;)"><span class="bold">一泓</span></span><span class="wordLink" onclick="ShowWordNote(4466, &quot;poem_title_4&quot;)"><span class="bold">怀旧</span></span><span class="wordLink" onclick="ShowWordNote(473549, &quot;poem_title_4&quot;)"><span class="bold">泪</span></span>，<span class="wordLink" onclick="ShowWordNote(617, &quot;poem_title_4&quot;)"><span class="bold">随风</span></span><span class="wordLink" onclick="ShowWordNote(476396, &quot;poem_title_4&quot;)"><span class="bold">散</span></span><span class="wordLink" onclick="ShowWordNote(472706, &quot;poem_title_4&quot;)"><span class="bold">作</span></span><span class="wordLink" onclick="ShowWordNote(473740, &quot;poem_title_4&quot;)"><span class="bold">雨</span></span><span class="wordLink" onclick="ShowWordNote(971, &quot;poem_title_4&quot;)"><span class="bold">霏霏</span></span>。<div id="poem_title_4_comment" style="display:block"></div>		<div class="ckzl">
   		<p>释义为系统自动检索，难免有误，仅供参考。</p><p>
   		</p></div>
   		</div>
   	</div>	
   	<div class="shici-pic">
   		<div class="shici-pic-box">
   			<div class="divimg" style="margin-top:2px;">
   				<a href="https://m.gushici.net/zuozhe/26/3117.html"><img src="./郭公辅挽诗原文_龚诩_古诗词网_files/gongxu.jpg" width="105" height="150" alt="龚诩简介"/></a>
   			</div>
   			<p style="height:24px;">
               <a style="font-size:20px; line-height:24px; height:24px;" href="https://m.gushici.net/zuozhe/26/3117.html"><b>龚诩</b></a></p>
               <p style=" margin:0px;">
   				龚诩（1381～1469）明代学者。一名翊，字大章，号纯庵，南直隶苏州府昆山（今属江苏）人。建文时为金川门卒，燕兵至，恸哭遁归，隐居授徒，后周忱巡抚江南，两荐为学官，坚辞，有《野古集》。<a href="https://m.gushici.net/t/16/16868/">龚诩的诗词&gt;&gt;</a>
   			</p>
   		</div>
   	</div>
   	<div class="title">
   		<div class="titleleft"></div>
   			猜您喜欢
   	</div>
   	<div class="gushici">
   		<div class="gushici-box">
   			<p><a style="font-size:20px; line-height:24px; height:24px;" href="https://m.gushici.net/shici/579/579408.html"><b>寄怀袁兄</b></a></p>
   			<p class="source"><a href="https://m.gushici.net/t/1/179/">明代</a><span>：</span><a href="https://m.gushici.net/t/16/16868/">龚诩</a></p>
   			<div class="gushici-box-text" style="position:relative; z-index:0px;">
   				双丸递走疾如梭，老色骎骎可奈何。<br/>白发尽从愁里得，青春都向客中过。<br/>壮怀久息营为懒，往事难忘感慨多。<br/>珍重故人知我意，停云回首付悲歌。</div>
   		</div>
   	</div>
   	<div class="gushici">
   		<div class="gushici-box">
   			<p><a style="font-size:20px; line-height:24px; height:24px;" href="https://m.gushici.net/shici/579/579407.html"><b>赵氏双桂轩</b></a></p>
   			<p class="source"><a href="https://m.gushici.net/t/1/179/">明代</a><span>：</span><a href="https://m.gushici.net/t/16/16868/">龚诩</a></p>
   			<div class="gushici-box-text" style="position:relative; z-index:0px;">
   				赵公二子美无度，双桂名轩拟弟兄。<br/>已信结根同得地，更怜交叶总含清。<br/>肯随桃李争颜色，要与松篁一性情。<br/>笑煞田家紫荆树，却从憔悴复重荣。</div>
   		</div>
   	</div>
   	<div class="gushici">
   		<div class="gushici-box">
   			<p><a style="font-size:20px; line-height:24px; height:24px;" href="https://m.gushici.net/shici/579/579406.html"><b>农乐轩为陆安道赋</b></a></p>
   			<p class="source"><a href="https://m.gushici.net/t/1/179/">明代</a><span>：</span><a href="https://m.gushici.net/t/16/16868/">龚诩</a></p>
   			<div class="gushici-box-text" style="position:relative; z-index:0px;">
   				耕耘虽苦非为苦，贫贱堪忧未足忧。<br/>老妇一生甘食藿，稚孙十岁解骑牛。<br/>官租已毕知无事，邻社相邀庆有秋。<br/>绝胜眼中金与紫，施施难免乞墦羞。			</div>
   		</div>
   	</div>
   </div>
   <div class="footer">
   	© 2020 <a href="https://www.gushici.net/">古诗词网</a> | <a href="https://beian.miit.gov.cn/" target="_blank">鲁ICP备20001911号-1</a>
   </div>
   <div style="display:none">
   <script>
   var _hmt = _hmt || [];
   (function() {
     var hm = document.createElement("script");
     hm.src = "https://hm.baidu.com/hm.js?b08b1655b77a8bba18cee23b7acf602c";
     var s = document.getElementsByTagName("script")[0]; 
     s.parentNode.insertBefore(hm, s);
   })();
   </script>
   </div>
   
   </body></html>
   ```

   ```python
   #python代码
   
   # 从lxml模块中导入etree,用于解析和遍历XML/HTML文档
   from lxml import etree
   
   if __name__ == '__main__':
   
       #实例化一个etree对象，且将被解析的本地源码文件加载到了该对象中
       tree = etree.parse('./郭公辅挽诗原文_龚诩_古诗词网.html')
       # ’/‘表示从根节点开始定位，表示一个层级，逐步的定位到body下的所有直系的div
       # r = tree.xpath('/html/body/div')
   
       # // 表示多个层级，可以从任意位置开始定位
       # r = tree.xpath('/html//div')
   
       # 属性定位，例如根据div的class来定位
       # r = tree.xpath('//div[@class="gushici"]')
   
       # 根据索引定位，例如div下面直系所有a标签下：a[1] （索引从1开始）
       # r = tree.xpath('//div[@class = "shici-pic-box"]/div/a[1]')
   
       # 获取文本，通过 /text() 获取直系下的文本  或者 //text() 获取所有的的文本
       # r = tree.xpath('//div[@class = "shici-pic-box"]/div/a[1]/text()')
   
       # 获取属性： /@attrName 例如 img src href
       # 获取属性:/@attrName,例如获取img标签的src属性:
       # img_src = tree.xpath('//img/@src')
       r = tree.xpath('//div[@class = "shici-pic-box"]/div/a[1]/@href')
       
       print(r)    # 打印获取到的href属性值
   ```

   

